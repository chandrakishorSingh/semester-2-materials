

\documentclass{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, fancyhdr, color, comment, graphicx, environ}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage{indentfirst}
\usepackage{hyperref}
\renewcommand{\footrulewidth}{0.8pt}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}


\pagestyle{fancy}



\newenvironment{problem}[2][Problem]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}


\newenvironment{solution}{\textbf{Solution}}


\lhead{MIT2021117}
\rhead{Convex Optimizations}
\chead{\textbf{Assignment 6}}
\lfoot{}
\rfoot{}


\begin{document}

    \begin{problem}{1}
    Consider the function $f(x,y) = x^2 + y^2 + \beta xy + x + 2y$. For what values of $\beta$, does this function have a unique global minimum ?
    \end{problem}
    
    \begin{solution}
    
    A function have to be strictly convex for it to have unique global minima.
    
    \hfill
    
    \[
        \nabla f = \begin{bmatrix}
                    \frac{\delta f}{\delta x} \\
                    \frac{\delta f}{\delta y}
                    \end{bmatrix}
    \]
    
    \[
        = \begin{bmatrix}
        2x + \beta y + 1 \\
        2y + \beta x + 2
        \end{bmatrix}
    \]
    
    Hessian, H = $
        \begin{bmatrix}
        \frac{\delta^2 f}{\delta x^2} & \frac{\delta^2 f}{\delta y \delta x} \\
        \frac{\delta^2 f}{\delta x \delta y} & \frac{\delta^2 f}{\delta y^2}
        \end{bmatrix}
    $
    $
        = \begin{bmatrix}
        2 & \beta \\
        \beta & 2
        \end{bmatrix}
    $
    $
        = 4 - \beta^2
    $
    
    \hfill
    
    Hessian must be greater than zero for function to be strictly convex.
    
    \hfill
    
    So, $4 - \beta^2 > 0 \implies \beta^2 < 4$
    \end{solution}
    
    \begin{problem}{2}
        Suppose the one dimensional function $f(x_{k} + \alpha d_{k})$ is unimodal and differentiable. Let $\alpha^{*}$ be the minimum of the function. If any $\alpha \geq \alpha^{*}$ is selected, show that $\nabla f(x_{k + 1})^{T}d_{k} > 0$
    \end{problem}
    
    \begin{solution}
        let $t(\alpha) = f(x_{k} + \alpha d_{k})$.
        
        \hfill
        
        Since, $t(\alpha)$ is unimodal $t(\alpha)$ has one global minima and one local minima.
        
        \hfill
        
        So as $\alpha^{*}$ is the minimum of the function then for any $\alpha > \alpha^{*}$.
        
        \hfill
        
        Hence, $\frac{dt}{d\alpha} > 0$
        
        \hfill
        
        $\frac{dt}{d\alpha} = \frac{d}{d \alpha} (f(x_{k} + \alpha d_{k})) = \nabla f^{T}_{k + 1} d_{k}$
        
        \hfill
        
        As $\frac{dt}{d\alpha} > 0$, So, $\nabla f(x_{k + 1}^{T} d_{k}) > 0$
        
        \hfill
        
    \end{solution}
    
    \begin{problem}{3}
        Consider the following problem. $f(x, y) = exp(-\frac{1}{3} x^{3} + x - y^{2})$. Suppose you want to do it using pure Newtonâ€™s method. Is $x_{0} = (-1, 1)$ good starting point ?
    \end{problem}
    
    \begin{solution}
        Newton method can be applied only in decent direction. So, we formulate the problem as minimize $f(x, y) = -exp(-\frac{1}{3} x^{3} + x - y^{2})$
        
        \hfill
        
        $\frac{\delta f}{\delta x} = (-x^{2} + 1) exp(-\frac{1}{3} x^{3} + x - y^{2})$
        
        \hfill
        
        $\frac{\delta f}{\delta y} = -2y exp(-\frac{1}{3} x^{3} + x - y^{2})$
        
        \hfill
        
        $\frac{\delta^2 f}{\delta x^2} = (-x^{2} + 1)^2 exp(-\frac{1}{3} x^{3} + x - y^{2}) - 2x exp(-\frac{1}{3} x^{3} + x - y^{2})$
        
        \hfill
        
        $\frac{\delta^2 f}{\delta y^2} = -2 exp(-\frac{1}{3} x^{3} + x - y^{2}) + 4y^2 exp(-\frac{1}{3} x^{3} + x - y^{2})$
        
        \hfill
        
        $\frac{\delta^2 f}{\delta y \delta x} = -2y(-x^2 + 1) exp(-\frac{1}{3} x^{3} + x - y^{2})$
        
        \hfill
        
        $\frac{\delta^2 f}{\delta x \delta y} = -2y(-x^2 + 1) exp(-\frac{1}{3} x^{3} + x - y^{2})$
        
        Hessian, H = $
        \begin{bmatrix}
        \frac{\delta^2 f}{\delta x^2} & \frac{\delta^2 f}{\delta y \delta x} \\
        \frac{\delta^2 f}{\delta x \delta y} & \frac{\delta^2 f}{\delta y^2}
        \end{bmatrix}
        $
        \[
            =
            \begin{bmatrix}
            (-x^{2} + 1)^2 exp(-\frac{1}{3} x^{3} + x - y^{2}) - 2x exp(-\frac{1}{3} x^{3} + x - y^{2}) & -2y(-x^2 + 1) exp(-\frac{1}{3} x^{3} + x - y^{2}) \\
            
            -2y(-x^2 + 1) exp(-\frac{1}{3} x^{3} + x - y^{2}) & -2 exp(-\frac{1}{3} x^{3} + x - y^{2}) + 4y^2 exp(-\frac{1}{3} x^{3} + x - y^{2})
            \end{bmatrix}
        \]
        
        $H_{x_{0}} = 
            \begin{bmatrix}
                -2 exp(-5/2) & 0 \\
                0   & -2exp(-5/2)
            \end{bmatrix}
        $
        
        \hfill
        
        Since this is a negative definite matrix, hence $X_{0}$ is not a good starting point.
    \end{solution}
    
    \begin{problem}{4}
        Find the rectangle of given perimeter that has greatest area by using Lagrange multiplier theorem.
Verify it using second order conditions.
    \end{problem}
    
    \begin{solution}
        Let $x$ and y be the length and width of the rectangle. The given perimeter is $x + y = c$ where $c$ is given. So, the problem becomes
        
        \hfill
        
        maximize $z = xy$
        
        subject to $x + y = c$
        
        \hfill
        
        We can rewrite the problem as 
        
        \hfill
        
        minimize $z = -xy$
        
        subject to $x + y = c$
        
        \hfill
        
        So, the lagrangian problem becomes
        
        $L(\lambda) = -xy + \lambda(x + y - c)$
        
        \hfill
        
        $\frac{\delta L}{\delta x} = -y + \lambda$
        
        \hfill
        
        $\frac{\delta L}{\delta y} = -x + \lambda$
        
        \hfill
        
        $\frac{\delta L}{\delta \lambda} = x + y -c$
        
        \hfill
        
        $\frac{\delta L}{\delta x} = 0 \implies y = \lambda$
        
        \hfill
        
        $\frac{\delta L}{\delta y} = 0 \implies x = \lambda$
        
        \hfill
        
        $\frac{\delta L}{\delta \lambda} = 0 \implies x + y = c \implies x = y = c / 2$
        
        \hfill
        
        So, $x^* = y^* = c / 2$
        
        Border Hessian Matrix $H^{B} = \begin{bmatrix}
            0 & g_{x} & g_{y} \\
            g_{x} & L_{xx} & L_{xy} \\
            g_{y} & L_{yx} & L_{yy} \\
        \end{bmatrix}$
        
        $= \begin{bmatrix}
            0 & 1 & 1 \\
            1 & 0 & -1 \\
            1 & -1 & 0
        \end{bmatrix}$
        
        $= -2$
        
        Since $|H^B| < 0$, so the objective function is minimized.
        
    \end{solution}
    
    \newpage
    
    \begin{problem}{5}
        Consider
        
        Minimize $f(x,y) = -x$
        
        Subject to
        $y - (1 - x^3)$
        $-y \leq 0$
        
        a. Find the optimal solution by solving it graphically.
        
        b. Do Lagrange multiplier exist ? How could you say without actually solving the problem ?
    \end{problem}
    
    \begin{solution}
        a) Ploted the graph using an online tool which looks like below
        
        \hfill
        
        \includegraphics[scale=.75]{1.png}
        
        So, the solution is $X^* = \begin{bmatrix}
            1 \\
            0
        \end{bmatrix}$

        b) Here, $\Delta s_{1}, \Delta s_{2}$ are linearly dependent. So lagrange multiplier does not exists.
        
    \end{solution}

\end{document}
